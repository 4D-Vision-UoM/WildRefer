BeaUTyDETR(
  (point_backbone_net): Pointnet2Backbone(
    (sa1): PointnetSAModuleVotes(
      (grouper): QueryAndGroup()
      (mlp_module): SharedMLP(
        (layer0): Conv2d(
          (conv): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
        (layer1): Conv2d(
          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
        (layer2): Conv2d(
          (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
      )
    )
    (sa2): PointnetSAModuleVotes(
      (grouper): QueryAndGroup()
      (mlp_module): SharedMLP(
        (layer0): Conv2d(
          (conv): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
        (layer1): Conv2d(
          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
        (layer2): Conv2d(
          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
      )
    )
    (sa3): PointnetSAModuleVotes(
      (grouper): QueryAndGroup()
      (mlp_module): SharedMLP(
        (layer0): Conv2d(
          (conv): Conv2d(259, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
        (layer1): Conv2d(
          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
        (layer2): Conv2d(
          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
      )
    )
    (sa4): PointnetSAModuleVotes(
      (grouper): QueryAndGroup()
      (mlp_module): SharedMLP(
        (layer0): Conv2d(
          (conv): Conv2d(259, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
        (layer1): Conv2d(
          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
        (layer2): Conv2d(
          (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
      )
    )
    (fp1): PointnetFPModule(
      (mlp): SharedMLP(
        (layer0): Conv2d(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
        (layer1): Conv2d(
          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
      )
    )
    (fp2): PointnetFPModule(
      (mlp): SharedMLP(
        (layer0): Conv2d(
          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
        (layer1): Conv2d(
          (conv): Conv2d(256, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): BatchNorm2d(
            (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (activation): ReLU(inplace=True)
        )
      )
    )
  )
  (image_backbone_net): VisualBackbone(
    (body): IntermediateLayerGetter(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): FrozenBatchNorm2d()
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
        (1): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
        (2): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
      )
      (layer2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (downsample): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d()
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
        (2): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
        (3): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
      )
      (layer3): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (downsample): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d()
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
        (2): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
        (3): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
        (4): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
        (5): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
      )
      (layer4): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d()
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
        (2): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
        )
      )
    )
    (proj): Conv2d(512, 288, kernel_size=(1, 1), stride=(1, 1))
    (position_embedding): PositionEmbeddingSine()
  )
  (multi_fuser): ModuleList(
    (0): MultiCALayer(
      (attn_modules): ModuleList(
        (0): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
        )
        (1): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
        )
      )
      (norm_modules): ModuleList(
        (0): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (dropout_modules): ModuleList(
        (0): Dropout(p=0.1, inplace=False)
        (1): Dropout(p=0.1, inplace=False)
      )
      (ffn): Sequential(
        (0): Linear(in_features=288, out_features=256, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=256, out_features=288, bias=True)
        (4): Dropout(p=0.1, inplace=False)
      )
      (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (self_posembed): PositionEmbeddingLearned(
        (position_embedding_head): Sequential(
          (0): Conv1d(3, 288, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(288, 288, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (image_multi_fuser): ModuleList(
    (0): ImageMultiCALayer(
      (attn_modules): ModuleList(
        (0): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
        )
        (1): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
        )
      )
      (norm_modules): ModuleList(
        (0): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      )
      (dropout_modules): ModuleList(
        (0): Dropout(p=0.1, inplace=False)
        (1): Dropout(p=0.1, inplace=False)
      )
      (ffn): Sequential(
        (0): Linear(in_features=288, out_features=256, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=256, out_features=288, bias=True)
        (4): Dropout(p=0.1, inplace=False)
      )
      (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
    )
  )
  (text_encoder): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (text_projector): Sequential(
    (0): Linear(in_features=768, out_features=288, bias=True)
    (1): LayerNorm((288,), eps=1e-12, elementwise_affine=True)
    (2): Dropout(p=0.1, inplace=False)
  )
  (pos_embed): PositionEmbeddingLearned(
    (position_embedding_head): Sequential(
      (0): Conv1d(3, 288, kernel_size=(1,), stride=(1,))
      (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv1d(288, 288, kernel_size=(1,), stride=(1,))
    )
  )
  (cross_encoder_text_points): BiEncoder(
    (layers): ModuleList(
      (0): BiEncoderLayer(
        (self_attention_lang): TransformerEncoderLayerNoFFN(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
        )
        (self_attention_visual): PosTransformerEncoderLayerNoFFN(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
        )
        (cross_layer): CrossAttentionLayer(
          (cross_lv): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (dropout_lv): Dropout(p=0.1, inplace=False)
          (norm_lv): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (ffn_lv): Sequential(
            (0): Linear(in_features=288, out_features=256, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=256, out_features=288, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
          (norm_lv2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (cross_vl): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (dropout_vl): Dropout(p=0.1, inplace=False)
          (norm_vl): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (ffn_vl): Sequential(
            (0): Linear(in_features=288, out_features=256, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=256, out_features=288, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
          (norm_vl2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (cross_d): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (dropout_d): Dropout(p=0.1, inplace=False)
          (norm_d): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BiEncoderLayer(
        (self_attention_lang): TransformerEncoderLayerNoFFN(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
        )
        (self_attention_visual): PosTransformerEncoderLayerNoFFN(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
        )
        (cross_layer): CrossAttentionLayer(
          (cross_lv): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (dropout_lv): Dropout(p=0.1, inplace=False)
          (norm_lv): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (ffn_lv): Sequential(
            (0): Linear(in_features=288, out_features=256, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=256, out_features=288, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
          (norm_lv2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (cross_vl): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (dropout_vl): Dropout(p=0.1, inplace=False)
          (norm_vl): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (ffn_vl): Sequential(
            (0): Linear(in_features=288, out_features=256, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=256, out_features=288, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
          (norm_vl2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (cross_d): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (dropout_d): Dropout(p=0.1, inplace=False)
          (norm_d): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BiEncoderLayer(
        (self_attention_lang): TransformerEncoderLayerNoFFN(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
        )
        (self_attention_visual): PosTransformerEncoderLayerNoFFN(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
        )
        (cross_layer): CrossAttentionLayer(
          (cross_lv): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (dropout_lv): Dropout(p=0.1, inplace=False)
          (norm_lv): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (ffn_lv): Sequential(
            (0): Linear(in_features=288, out_features=256, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=256, out_features=288, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
          (norm_lv2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (cross_vl): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (dropout_vl): Dropout(p=0.1, inplace=False)
          (norm_vl): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (ffn_vl): Sequential(
            (0): Linear(in_features=288, out_features=256, bias=True)
            (1): ReLU()
            (2): Dropout(p=0.1, inplace=False)
            (3): Linear(in_features=256, out_features=288, bias=True)
            (4): Dropout(p=0.1, inplace=False)
          )
          (norm_vl2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (cross_d): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
          )
          (dropout_d): Dropout(p=0.1, inplace=False)
          (norm_d): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (points_obj_cls): PointsObjClsModule(
    (conv1): Conv1d(288, 288, kernel_size=(1,), stride=(1,))
    (bn1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): Conv1d(288, 288, kernel_size=(1,), stride=(1,))
    (bn2): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): Conv1d(288, 1, kernel_size=(1,), stride=(1,))
  )
  (gsample_module): GeneralSamplingModule()
  (decoder_query_proj): Conv1d(288, 288, kernel_size=(1,), stride=(1,))
  (proposal_head): ClsAgnosticPredictHead(
    (center_residual_head): ThreeLayerMLP(
      (net): Sequential(
        (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
        (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Dropout(p=0.3, inplace=False)
        (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
        (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Dropout(p=0.3, inplace=False)
        (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
      )
    )
    (size_pred_head): ThreeLayerMLP(
      (net): Sequential(
        (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
        (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Dropout(p=0.3, inplace=False)
        (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
        (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Dropout(p=0.3, inplace=False)
        (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
      )
    )
    (sem_cls_scores_head): ThreeLayerMLP(
      (net): Sequential(
        (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
        (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Dropout(p=0.3, inplace=False)
        (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
        (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Dropout(p=0.3, inplace=False)
        (8): Conv1d(288, 100, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (decoder): ModuleList(
    (0): BiDecoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (cross_l): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (dropout_l): Dropout(p=0.1, inplace=False)
      (norm_l): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (cross_v): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (dropout_v): Dropout(p=0.1, inplace=False)
      (norm_v): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=288, out_features=256, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=256, out_features=288, bias=True)
        (4): Dropout(p=0.1, inplace=False)
      )
      (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (self_posembed): PositionEmbeddingLearned(
        (position_embedding_head): Sequential(
          (0): Conv1d(6, 288, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(288, 288, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): BiDecoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (cross_l): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (dropout_l): Dropout(p=0.1, inplace=False)
      (norm_l): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (cross_v): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (dropout_v): Dropout(p=0.1, inplace=False)
      (norm_v): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=288, out_features=256, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=256, out_features=288, bias=True)
        (4): Dropout(p=0.1, inplace=False)
      )
      (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (self_posembed): PositionEmbeddingLearned(
        (position_embedding_head): Sequential(
          (0): Conv1d(6, 288, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(288, 288, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): BiDecoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (cross_l): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (dropout_l): Dropout(p=0.1, inplace=False)
      (norm_l): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (cross_v): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (dropout_v): Dropout(p=0.1, inplace=False)
      (norm_v): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=288, out_features=256, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=256, out_features=288, bias=True)
        (4): Dropout(p=0.1, inplace=False)
      )
      (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (self_posembed): PositionEmbeddingLearned(
        (position_embedding_head): Sequential(
          (0): Conv1d(6, 288, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(288, 288, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (3): BiDecoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (cross_l): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (dropout_l): Dropout(p=0.1, inplace=False)
      (norm_l): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (cross_v): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (dropout_v): Dropout(p=0.1, inplace=False)
      (norm_v): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=288, out_features=256, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=256, out_features=288, bias=True)
        (4): Dropout(p=0.1, inplace=False)
      )
      (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (self_posembed): PositionEmbeddingLearned(
        (position_embedding_head): Sequential(
          (0): Conv1d(6, 288, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(288, 288, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (4): BiDecoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (cross_l): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (dropout_l): Dropout(p=0.1, inplace=False)
      (norm_l): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (cross_v): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (dropout_v): Dropout(p=0.1, inplace=False)
      (norm_v): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=288, out_features=256, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=256, out_features=288, bias=True)
        (4): Dropout(p=0.1, inplace=False)
      )
      (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (self_posembed): PositionEmbeddingLearned(
        (position_embedding_head): Sequential(
          (0): Conv1d(6, 288, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(288, 288, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (5): BiDecoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (cross_l): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (dropout_l): Dropout(p=0.1, inplace=False)
      (norm_l): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (cross_v): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=288, out_features=288, bias=True)
      )
      (dropout_v): Dropout(p=0.1, inplace=False)
      (norm_v): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (ffn): Sequential(
        (0): Linear(in_features=288, out_features=256, bias=True)
        (1): ReLU()
        (2): Dropout(p=0.1, inplace=False)
        (3): Linear(in_features=256, out_features=288, bias=True)
        (4): Dropout(p=0.1, inplace=False)
      )
      (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
      (self_posembed): PositionEmbeddingLearned(
        (position_embedding_head): Sequential(
          (0): Conv1d(6, 288, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv1d(288, 288, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (prediction_heads): ModuleList(
    (0): ClsAgnosticPredictHead(
      (center_residual_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
        )
      )
      (size_pred_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
        )
      )
      (sem_cls_scores_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 100, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (1): ClsAgnosticPredictHead(
      (center_residual_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
        )
      )
      (size_pred_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
        )
      )
      (sem_cls_scores_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 100, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (2): ClsAgnosticPredictHead(
      (center_residual_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
        )
      )
      (size_pred_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
        )
      )
      (sem_cls_scores_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 100, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (3): ClsAgnosticPredictHead(
      (center_residual_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
        )
      )
      (size_pred_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
        )
      )
      (sem_cls_scores_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 100, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (4): ClsAgnosticPredictHead(
      (center_residual_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
        )
      )
      (size_pred_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
        )
      )
      (sem_cls_scores_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 100, kernel_size=(1,), stride=(1,))
        )
      )
    )
    (5): ClsAgnosticPredictHead(
      (center_residual_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
        )
      )
      (size_pred_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 3, kernel_size=(1,), stride=(1,))
        )
      )
      (sem_cls_scores_head): ThreeLayerMLP(
        (net): Sequential(
          (0): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (1): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Dropout(p=0.3, inplace=False)
          (4): Conv1d(288, 288, kernel_size=(1,), stride=(1,), bias=False)
          (5): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (6): ReLU()
          (7): Dropout(p=0.3, inplace=False)
          (8): Conv1d(288, 100, kernel_size=(1,), stride=(1,))
        )
      )
    )
  )
  (contrastive_align_projection_image): Sequential(
    (0): Linear(in_features=288, out_features=288, bias=True)
    (1): ReLU()
    (2): Linear(in_features=288, out_features=288, bias=True)
    (3): ReLU()
    (4): Linear(in_features=288, out_features=64, bias=True)
  )
  (contrastive_align_projection_text): Sequential(
    (0): Linear(in_features=288, out_features=288, bias=True)
    (1): ReLU()
    (2): Linear(in_features=288, out_features=288, bias=True)
    (3): ReLU()
    (4): Linear(in_features=288, out_features=64, bias=True)
  )
)