/home/avishka/anaconda3/envs/wildrefer_env/lib/python3.8/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).

================================================================================
BATCH INFORMATION (before taking single sample)
================================================================================
Original batch point_clouds shape: torch.Size([32, 2, 30000, 6])
  Batch contains 32 samples
  Each sample has 2 points
  Each point has 30000 features (XYZ + features)
================================================================================

dict_keys(['point_clouds', 'text', 'dynamic_mask', 'image', 'img_mask', 'center_label', 'size_gts', 'box_label_mask', 'point_instance_label', 'sem_cls_label', 'tokens_positive', 'positive_map'])

================================================================================
ORIGINAL INPUT SHAPES BEFORE PREPROCESSING
================================================================================
[point_clouds]: torch.Size([1, 2, 30000, 6]) (dtype: torch.float32, device: cuda:0)
[text]: list
[dynamic_mask]: torch.Size([1, 2]) (dtype: torch.int64, device: cuda:0)
[image]: torch.Size([1, 2, 3, 384, 384]) (dtype: torch.float32, device: cuda:0)
[img_mask]: torch.Size([1, 2, 384, 384]) (dtype: torch.bool, device: cuda:0)
[center_label]: torch.Size([1, 100, 3]) (dtype: torch.float32, device: cuda:0)
[size_gts]: torch.Size([1, 100, 3]) (dtype: torch.float32, device: cuda:0)
[box_label_mask]: torch.Size([1, 100]) (dtype: torch.float32, device: cuda:0)
[point_instance_label]: torch.Size([1, 30000]) (dtype: torch.int64, device: cuda:0)
[sem_cls_label]: torch.Size([1, 100]) (dtype: torch.int64, device: cuda:0)
[tokens_positive]: torch.Size([1, 100, 2]) (dtype: torch.int64, device: cuda:0)
[positive_map]: torch.Size([1, 100, 100]) (dtype: torch.float32, device: cuda:0)

Point Cloud Details:
  Shape: torch.Size([1, 2, 30000, 6])
  Batch size: 1
  Number of points per sample: 2
  Number of features per point: 30000
  Min value: -16.8507
  Max value: 24.9921
  Mean value: 1.5694
================================================================================


================================================================================
REGISTERING FORWARD HOOKS ON ALL MODULES
================================================================================


================================================================================
STARTING FORWARD PASS
================================================================================

[point_backbone_net.sa1.grouper] Output shapes (tuple): ['torch.Size([2, 6, 2048, 64])', 'torch.Size([2, 3, 2048, 64])']
[point_backbone_net.sa1.mlp_module.layer0.conv] Output shape: torch.Size([2, 64, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer0.bn.bn] Output shape: torch.Size([2, 64, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer0.bn] Output shape: torch.Size([2, 64, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 64, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer0] Output shape: torch.Size([2, 64, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer1.conv] Output shape: torch.Size([2, 64, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer1.bn.bn] Output shape: torch.Size([2, 64, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer1.bn] Output shape: torch.Size([2, 64, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 64, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer1] Output shape: torch.Size([2, 64, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer2.conv] Output shape: torch.Size([2, 128, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer2.bn.bn] Output shape: torch.Size([2, 128, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer2.bn] Output shape: torch.Size([2, 128, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 128, 2048, 64])
[point_backbone_net.sa1.mlp_module.layer2] Output shape: torch.Size([2, 128, 2048, 64])
[point_backbone_net.sa1.mlp_module] Output shape: torch.Size([2, 128, 2048, 64])
[point_backbone_net.sa1] Output shapes (tuple): ['torch.Size([2, 2048, 3])', 'torch.Size([2, 128, 2048])', 'torch.Size([2, 2048])']
[point_backbone_net.sa2.grouper] Output shapes (tuple): ['torch.Size([2, 131, 1024, 32])', 'torch.Size([2, 3, 1024, 32])']
[point_backbone_net.sa2.mlp_module.layer0.conv] Output shape: torch.Size([2, 128, 1024, 32])
[point_backbone_net.sa2.mlp_module.layer0.bn.bn] Output shape: torch.Size([2, 128, 1024, 32])
[point_backbone_net.sa2.mlp_module.layer0.bn] Output shape: torch.Size([2, 128, 1024, 32])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 128, 1024, 32])
[point_backbone_net.sa2.mlp_module.layer0] Output shape: torch.Size([2, 128, 1024, 32])
[point_backbone_net.sa2.mlp_module.layer1.conv] Output shape: torch.Size([2, 128, 1024, 32])
[point_backbone_net.sa2.mlp_module.layer1.bn.bn] Output shape: torch.Size([2, 128, 1024, 32])
[point_backbone_net.sa2.mlp_module.layer1.bn] Output shape: torch.Size([2, 128, 1024, 32])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 128, 1024, 32])
[point_backbone_net.sa2.mlp_module.layer1] Output shape: torch.Size([2, 128, 1024, 32])
[point_backbone_net.sa2.mlp_module.layer2.conv] Output shape: torch.Size([2, 256, 1024, 32])
[point_backbone_net.sa2.mlp_module.layer2.bn.bn] Output shape: torch.Size([2, 256, 1024, 32])
[point_backbone_net.sa2.mlp_module.layer2.bn] Output shape: torch.Size([2, 256, 1024, 32])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 256, 1024, 32])
[point_backbone_net.sa2.mlp_module.layer2] Output shape: torch.Size([2, 256, 1024, 32])
[point_backbone_net.sa2.mlp_module] Output shape: torch.Size([2, 256, 1024, 32])
[point_backbone_net.sa2] Output shapes (tuple): ['torch.Size([2, 1024, 3])', 'torch.Size([2, 256, 1024])', 'torch.Size([2, 1024])']
[point_backbone_net.sa3.grouper] Output shapes (tuple): ['torch.Size([2, 259, 512, 16])', 'torch.Size([2, 3, 512, 16])']
[point_backbone_net.sa3.mlp_module.layer0.conv] Output shape: torch.Size([2, 128, 512, 16])
[point_backbone_net.sa3.mlp_module.layer0.bn.bn] Output shape: torch.Size([2, 128, 512, 16])
[point_backbone_net.sa3.mlp_module.layer0.bn] Output shape: torch.Size([2, 128, 512, 16])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 128, 512, 16])
[point_backbone_net.sa3.mlp_module.layer0] Output shape: torch.Size([2, 128, 512, 16])
[point_backbone_net.sa3.mlp_module.layer1.conv] Output shape: torch.Size([2, 128, 512, 16])
[point_backbone_net.sa3.mlp_module.layer1.bn.bn] Output shape: torch.Size([2, 128, 512, 16])
[point_backbone_net.sa3.mlp_module.layer1.bn] Output shape: torch.Size([2, 128, 512, 16])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 128, 512, 16])
[point_backbone_net.sa3.mlp_module.layer1] Output shape: torch.Size([2, 128, 512, 16])
[point_backbone_net.sa3.mlp_module.layer2.conv] Output shape: torch.Size([2, 256, 512, 16])
[point_backbone_net.sa3.mlp_module.layer2.bn.bn] Output shape: torch.Size([2, 256, 512, 16])
[point_backbone_net.sa3.mlp_module.layer2.bn] Output shape: torch.Size([2, 256, 512, 16])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 256, 512, 16])
[point_backbone_net.sa3.mlp_module.layer2] Output shape: torch.Size([2, 256, 512, 16])
[point_backbone_net.sa3.mlp_module] Output shape: torch.Size([2, 256, 512, 16])
[point_backbone_net.sa3] Output shapes (tuple): ['torch.Size([2, 512, 3])', 'torch.Size([2, 256, 512])', 'torch.Size([2, 512])']
[point_backbone_net.sa4.grouper] Output shapes (tuple): ['torch.Size([2, 259, 256, 16])', 'torch.Size([2, 3, 256, 16])']
[point_backbone_net.sa4.mlp_module.layer0.conv] Output shape: torch.Size([2, 128, 256, 16])
[point_backbone_net.sa4.mlp_module.layer0.bn.bn] Output shape: torch.Size([2, 128, 256, 16])
[point_backbone_net.sa4.mlp_module.layer0.bn] Output shape: torch.Size([2, 128, 256, 16])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 128, 256, 16])
[point_backbone_net.sa4.mlp_module.layer0] Output shape: torch.Size([2, 128, 256, 16])
[point_backbone_net.sa4.mlp_module.layer1.conv] Output shape: torch.Size([2, 128, 256, 16])
[point_backbone_net.sa4.mlp_module.layer1.bn.bn] Output shape: torch.Size([2, 128, 256, 16])
[point_backbone_net.sa4.mlp_module.layer1.bn] Output shape: torch.Size([2, 128, 256, 16])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 128, 256, 16])
[point_backbone_net.sa4.mlp_module.layer1] Output shape: torch.Size([2, 128, 256, 16])
[point_backbone_net.sa4.mlp_module.layer2.conv] Output shape: torch.Size([2, 256, 256, 16])
[point_backbone_net.sa4.mlp_module.layer2.bn.bn] Output shape: torch.Size([2, 256, 256, 16])
[point_backbone_net.sa4.mlp_module.layer2.bn] Output shape: torch.Size([2, 256, 256, 16])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 256, 256, 16])
[point_backbone_net.sa4.mlp_module.layer2] Output shape: torch.Size([2, 256, 256, 16])
[point_backbone_net.sa4.mlp_module] Output shape: torch.Size([2, 256, 256, 16])
[point_backbone_net.sa4] Output shapes (tuple): ['torch.Size([2, 256, 3])', 'torch.Size([2, 256, 256])', 'torch.Size([2, 256])']
[point_backbone_net.fp1.mlp.layer0.conv] Output shape: torch.Size([2, 256, 512, 1])
[point_backbone_net.fp1.mlp.layer0.bn.bn] Output shape: torch.Size([2, 256, 512, 1])
[point_backbone_net.fp1.mlp.layer0.bn] Output shape: torch.Size([2, 256, 512, 1])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 256, 512, 1])
[point_backbone_net.fp1.mlp.layer0] Output shape: torch.Size([2, 256, 512, 1])
[point_backbone_net.fp1.mlp.layer1.conv] Output shape: torch.Size([2, 256, 512, 1])
[point_backbone_net.fp1.mlp.layer1.bn.bn] Output shape: torch.Size([2, 256, 512, 1])
[point_backbone_net.fp1.mlp.layer1.bn] Output shape: torch.Size([2, 256, 512, 1])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 256, 512, 1])
[point_backbone_net.fp1.mlp.layer1] Output shape: torch.Size([2, 256, 512, 1])
[point_backbone_net.fp1.mlp] Output shape: torch.Size([2, 256, 512, 1])
[point_backbone_net.fp1] Output shape: torch.Size([2, 256, 512])
[point_backbone_net.fp2.mlp.layer0.conv] Output shape: torch.Size([2, 256, 1024, 1])
[point_backbone_net.fp2.mlp.layer0.bn.bn] Output shape: torch.Size([2, 256, 1024, 1])
[point_backbone_net.fp2.mlp.layer0.bn] Output shape: torch.Size([2, 256, 1024, 1])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 256, 1024, 1])
[point_backbone_net.fp2.mlp.layer0] Output shape: torch.Size([2, 256, 1024, 1])
[point_backbone_net.fp2.mlp.layer1.conv] Output shape: torch.Size([2, 288, 1024, 1])
[point_backbone_net.fp2.mlp.layer1.bn.bn] Output shape: torch.Size([2, 288, 1024, 1])
[point_backbone_net.fp2.mlp.layer1.bn] Output shape: torch.Size([2, 288, 1024, 1])
[point_backbone_net.sa1.mlp_module.layer0.activation] Output shape: torch.Size([2, 288, 1024, 1])
[point_backbone_net.fp2.mlp.layer1] Output shape: torch.Size([2, 288, 1024, 1])
[point_backbone_net.fp2.mlp] Output shape: torch.Size([2, 288, 1024, 1])
[point_backbone_net.fp2] Output shape: torch.Size([2, 288, 1024])
[point_backbone_net] Output is dict with keys: ['sa1_inds', 'sa1_xyz', 'sa1_features', 'sa2_inds', 'sa2_xyz', 'sa2_features', 'sa3_xyz', 'sa3_features', 'sa4_xyz', 'sa4_features', 'fp2_features', 'fp2_xyz', 'fp2_inds']
  [point_backbone_net][sa1_inds]: torch.Size([2, 2048])
  [point_backbone_net][sa1_xyz]: torch.Size([2, 2048, 3])
  [point_backbone_net][sa1_features]: torch.Size([2, 128, 2048])
  [point_backbone_net][sa2_inds]: torch.Size([2, 1024])
  [point_backbone_net][sa2_xyz]: torch.Size([2, 1024, 3])
  [point_backbone_net][sa2_features]: torch.Size([2, 256, 1024])
  [point_backbone_net][sa3_xyz]: torch.Size([2, 512, 3])
  [point_backbone_net][sa3_features]: torch.Size([2, 256, 512])
  [point_backbone_net][sa4_xyz]: torch.Size([2, 256, 3])
  [point_backbone_net][sa4_features]: torch.Size([2, 256, 256])
  [point_backbone_net][fp2_features]: torch.Size([2, 288, 1024])
  [point_backbone_net][fp2_xyz]: torch.Size([2, 1024, 3])
  [point_backbone_net][fp2_inds]: torch.Size([2, 1024])
[image_backbone_net.body.conv1] Output shape: torch.Size([2, 64, 192, 192])
[image_backbone_net.body.bn1] Output shape: torch.Size([2, 64, 192, 192])
[image_backbone_net.body.relu] Output shape: torch.Size([2, 64, 192, 192])
[image_backbone_net.body.maxpool] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.0.conv1] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.0.bn1] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.0.relu] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.0.conv2] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.0.bn2] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.0.relu] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.0] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.1.conv1] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.1.bn1] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.1.relu] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.1.conv2] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.1.bn2] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.1.relu] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.1] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.2.conv1] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.2.bn1] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.2.relu] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.2.conv2] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.2.bn2] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.2.relu] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1.2] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer1] Output shape: torch.Size([2, 64, 96, 96])
[image_backbone_net.body.layer2.0.conv1] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.0.bn1] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.0.relu] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.0.conv2] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.0.bn2] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.0.downsample.0] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.0.downsample.1] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.0.downsample] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.0.relu] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.0] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.1.conv1] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.1.bn1] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.1.relu] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.1.conv2] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.1.bn2] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.1.relu] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.1] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.2.conv1] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.2.bn1] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.2.relu] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.2.conv2] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.2.bn2] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.2.relu] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.2] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.3.conv1] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.3.bn1] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.3.relu] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.3.conv2] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.3.bn2] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.3.relu] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2.3] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer2] Output shape: torch.Size([2, 128, 48, 48])
[image_backbone_net.body.layer3.0.conv1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.0.bn1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.0.relu] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.0.conv2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.0.bn2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.0.downsample.0] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.0.downsample.1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.0.downsample] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.0.relu] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.0] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.1.conv1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.1.bn1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.1.relu] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.1.conv2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.1.bn2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.1.relu] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.2.conv1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.2.bn1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.2.relu] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.2.conv2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.2.bn2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.2.relu] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.3.conv1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.3.bn1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.3.relu] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.3.conv2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.3.bn2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.3.relu] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.3] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.4.conv1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.4.bn1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.4.relu] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.4.conv2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.4.bn2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.4.relu] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.4] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.5.conv1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.5.bn1] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.5.relu] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.5.conv2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.5.bn2] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.5.relu] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3.5] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer3] Output shape: torch.Size([2, 256, 24, 24])
[image_backbone_net.body.layer4.0.conv1] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.0.bn1] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.0.relu] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.0.conv2] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.0.bn2] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.0.downsample.0] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.0.downsample.1] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.0.downsample] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.0.relu] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.0] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.1.conv1] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.1.bn1] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.1.relu] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.1.conv2] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.1.bn2] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.1.relu] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.1] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.2.conv1] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.2.bn1] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.2.relu] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.2.conv2] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.2.bn2] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.2.relu] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4.2] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body.layer4] Output shape: torch.Size([2, 512, 12, 12])
[image_backbone_net.body] Output is dict with keys: ['0']
  [image_backbone_net.body][0]: torch.Size([2, 512, 12, 12])
[image_backbone_net.proj] Output shape: torch.Size([2, 288, 12, 12])
[image_backbone_net.position_embedding] Output shape: torch.Size([2, 288, 12, 12])
[image_backbone_net] Output is dict with keys: ['sa1_inds', 'sa1_xyz', 'sa1_features', 'sa2_inds', 'sa2_xyz', 'sa2_features', 'sa3_xyz', 'sa3_features', 'sa4_xyz', 'sa4_features', 'fp2_features', 'fp2_xyz', 'fp2_inds', 'seed_inds', 'seed_xyz', 'seed_features', 'additional_seed_inds', 'additional_seed_xyz', 'additional_seed_features', 'image_feature', 'img_mask', 'img_pos']
  [image_backbone_net][sa1_inds]: torch.Size([2, 2048])
  [image_backbone_net][sa1_xyz]: torch.Size([2, 2048, 3])
  [image_backbone_net][sa1_features]: torch.Size([2, 128, 2048])
  [image_backbone_net][sa2_inds]: torch.Size([2, 1024])
  [image_backbone_net][sa2_xyz]: torch.Size([2, 1024, 3])
  [image_backbone_net][sa2_features]: torch.Size([2, 256, 1024])
  [image_backbone_net][sa3_xyz]: torch.Size([2, 512, 3])
  [image_backbone_net][sa3_features]: torch.Size([2, 256, 512])
  [image_backbone_net][sa4_xyz]: torch.Size([2, 256, 3])
  [image_backbone_net][sa4_features]: torch.Size([2, 256, 256])
  [image_backbone_net][fp2_features]: torch.Size([1, 288, 1024])
  [image_backbone_net][fp2_xyz]: torch.Size([1, 1024, 3])
  [image_backbone_net][fp2_inds]: torch.Size([1, 1024])
  [image_backbone_net][seed_inds]: torch.Size([1, 1024])
  [image_backbone_net][seed_xyz]: torch.Size([1, 1024, 3])
  [image_backbone_net][seed_features]: torch.Size([1, 288, 1024])
  [image_backbone_net][additional_seed_inds]: torch.Size([1, 2, 1024])
  [image_backbone_net][additional_seed_xyz]: torch.Size([1, 2, 1024, 3])
  [image_backbone_net][additional_seed_features]: torch.Size([1, 2, 288, 1024])
  [image_backbone_net][image_feature]: torch.Size([2, 288, 144])
  [image_backbone_net][img_mask]: torch.Size([2, 144])
  [image_backbone_net][img_pos]: torch.Size([2, 288, 144])
[text_encoder.embeddings.word_embeddings] Output shape: torch.Size([1, 33, 768])
[text_encoder.embeddings.token_type_embeddings] Output shape: torch.Size([1, 33, 768])
[text_encoder.embeddings.position_embeddings] Output shape: torch.Size([1, 33, 768])
[text_encoder.embeddings.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.embeddings.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.embeddings] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.0.attention.self.query] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.0.attention.self.key] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.0.attention.self.value] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.0.attention.self.dropout] Output shape: torch.Size([1, 12, 33, 33])
[text_encoder.encoder.layer.0.attention.self] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.0.attention.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.0.attention.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.0.attention.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.0.attention.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.0.attention] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.0.intermediate.dense] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.0.intermediate.intermediate_act_fn] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.0.intermediate] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.0.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.0.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.0.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.0.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.0] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.1.attention.self.query] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.1.attention.self.key] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.1.attention.self.value] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.1.attention.self.dropout] Output shape: torch.Size([1, 12, 33, 33])
[text_encoder.encoder.layer.1.attention.self] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.1.attention.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.1.attention.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.1.attention.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.1.attention.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.1.attention] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.1.intermediate.dense] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.1.intermediate.intermediate_act_fn] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.1.intermediate] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.1.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.1.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.1.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.1.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.1] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.2.attention.self.query] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.2.attention.self.key] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.2.attention.self.value] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.2.attention.self.dropout] Output shape: torch.Size([1, 12, 33, 33])
[text_encoder.encoder.layer.2.attention.self] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.2.attention.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.2.attention.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.2.attention.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.2.attention.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.2.attention] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.2.intermediate.dense] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.2.intermediate.intermediate_act_fn] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.2.intermediate] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.2.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.2.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.2.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.2.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.2] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.3.attention.self.query] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.3.attention.self.key] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.3.attention.self.value] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.3.attention.self.dropout] Output shape: torch.Size([1, 12, 33, 33])
[text_encoder.encoder.layer.3.attention.self] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.3.attention.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.3.attention.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.3.attention.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.3.attention.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.3.attention] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.3.intermediate.dense] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.3.intermediate.intermediate_act_fn] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.3.intermediate] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.3.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.3.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.3.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.3.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.3] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.4.attention.self.query] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.4.attention.self.key] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.4.attention.self.value] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.4.attention.self.dropout] Output shape: torch.Size([1, 12, 33, 33])
[text_encoder.encoder.layer.4.attention.self] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.4.attention.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.4.attention.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.4.attention.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.4.attention.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.4.attention] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.4.intermediate.dense] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.4.intermediate.intermediate_act_fn] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.4.intermediate] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.4.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.4.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.4.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.4.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.4] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.5.attention.self.query] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.5.attention.self.key] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.5.attention.self.value] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.5.attention.self.dropout] Output shape: torch.Size([1, 12, 33, 33])
[text_encoder.encoder.layer.5.attention.self] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.5.attention.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.5.attention.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.5.attention.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.5.attention.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.5.attention] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.5.intermediate.dense] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.5.intermediate.intermediate_act_fn] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.5.intermediate] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.5.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.5.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.5.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.5.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.5] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.6.attention.self.query] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.6.attention.self.key] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.6.attention.self.value] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.6.attention.self.dropout] Output shape: torch.Size([1, 12, 33, 33])
[text_encoder.encoder.layer.6.attention.self] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.6.attention.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.6.attention.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.6.attention.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.6.attention.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.6.attention] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.6.intermediate.dense] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.6.intermediate.intermediate_act_fn] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.6.intermediate] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.6.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.6.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.6.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.6.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.6] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.7.attention.self.query] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.7.attention.self.key] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.7.attention.self.value] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.7.attention.self.dropout] Output shape: torch.Size([1, 12, 33, 33])
[text_encoder.encoder.layer.7.attention.self] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.7.attention.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.7.attention.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.7.attention.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.7.attention.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.7.attention] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.7.intermediate.dense] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.7.intermediate.intermediate_act_fn] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.7.intermediate] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.7.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.7.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.7.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.7.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.7] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.8.attention.self.query] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.8.attention.self.key] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.8.attention.self.value] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.8.attention.self.dropout] Output shape: torch.Size([1, 12, 33, 33])
[text_encoder.encoder.layer.8.attention.self] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.8.attention.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.8.attention.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.8.attention.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.8.attention.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.8.attention] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.8.intermediate.dense] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.8.intermediate.intermediate_act_fn] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.8.intermediate] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.8.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.8.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.8.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.8.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.8] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.9.attention.self.query] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.9.attention.self.key] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.9.attention.self.value] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.9.attention.self.dropout] Output shape: torch.Size([1, 12, 33, 33])
[text_encoder.encoder.layer.9.attention.self] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.9.attention.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.9.attention.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.9.attention.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.9.attention.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.9.attention] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.9.intermediate.dense] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.9.intermediate.intermediate_act_fn] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.9.intermediate] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.9.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.9.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.9.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.9.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.9] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.10.attention.self.query] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.10.attention.self.key] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.10.attention.self.value] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.10.attention.self.dropout] Output shape: torch.Size([1, 12, 33, 33])
[text_encoder.encoder.layer.10.attention.self] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.10.attention.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.10.attention.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.10.attention.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.10.attention.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.10.attention] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.10.intermediate.dense] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.10.intermediate.intermediate_act_fn] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.10.intermediate] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.10.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.10.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.10.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.10.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.10] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.11.attention.self.query] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.11.attention.self.key] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.11.attention.self.value] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.11.attention.self.dropout] Output shape: torch.Size([1, 12, 33, 33])
[text_encoder.encoder.layer.11.attention.self] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.11.attention.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.11.attention.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.11.attention.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.11.attention.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.11.attention] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder.layer.11.intermediate.dense] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.11.intermediate.intermediate_act_fn] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.11.intermediate] Output shape: torch.Size([1, 33, 3072])
[text_encoder.encoder.layer.11.output.dense] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.11.output.dropout] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.11.output.LayerNorm] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.11.output] Output shape: torch.Size([1, 33, 768])
[text_encoder.encoder.layer.11] Output shapes (tuple): ['torch.Size([1, 33, 768])']
[text_encoder.encoder] Output is dict with keys: ['last_hidden_state']
  [text_encoder.encoder][last_hidden_state]: torch.Size([1, 33, 768])
[text_encoder.pooler.dense] Output shape: torch.Size([1, 768])
[text_encoder.pooler.activation] Output shape: torch.Size([1, 768])
[text_encoder.pooler] Output shape: torch.Size([1, 768])
[text_encoder] Output is dict with keys: ['last_hidden_state', 'pooler_output']
  [text_encoder][last_hidden_state]: torch.Size([1, 33, 768])
  [text_encoder][pooler_output]: torch.Size([1, 768])
[text_projector.0] Output shape: torch.Size([1, 33, 288])
[text_projector.1] Output shape: torch.Size([1, 33, 288])
[text_projector.2] Output shape: torch.Size([1, 33, 288])
[text_projector] Output shape: torch.Size([1, 33, 288])
[multi_fuser.0.self_posembed.position_embedding_head.0] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.1] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.2] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.3] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.0] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.1] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.2] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.3] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.0] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.1] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.2] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.3] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.attn_modules.0] Output shapes (tuple): ['torch.Size([1024, 1, 288])', 'torch.Size([1, 1024, 1024])']
[multi_fuser.0.dropout_modules.0] Output shape: torch.Size([1024, 1, 288])
[multi_fuser.0.norm_modules.0] Output shape: torch.Size([1024, 1, 288])
[multi_fuser.0.self_posembed.position_embedding_head.0] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.1] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.2] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.3] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.0] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.1] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.2] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head.3] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed.position_embedding_head] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.self_posembed] Output shape: torch.Size([1, 288, 1024])
[multi_fuser.0.attn_modules.1] Output shapes (tuple): ['torch.Size([1024, 1, 288])', 'torch.Size([1, 1024, 1024])']
[multi_fuser.0.dropout_modules.1] Output shape: torch.Size([1024, 1, 288])
[multi_fuser.0.norm_modules.1] Output shape: torch.Size([1024, 1, 288])
[multi_fuser.0.ffn.0] Output shape: torch.Size([1024, 1, 256])
[multi_fuser.0.ffn.1] Output shape: torch.Size([1024, 1, 256])
[multi_fuser.0.ffn.2] Output shape: torch.Size([1024, 1, 256])
[multi_fuser.0.ffn.3] Output shape: torch.Size([1024, 1, 288])
[multi_fuser.0.ffn.4] Output shape: torch.Size([1024, 1, 288])
[multi_fuser.0.ffn] Output shape: torch.Size([1024, 1, 288])
[multi_fuser.0.norm2] Output shape: torch.Size([1024, 1, 288])
[multi_fuser.0] Output shape: torch.Size([1, 288, 1024])
[image_multi_fuser.0.attn_modules.0] Output shapes (tuple): ['torch.Size([144, 1, 288])', 'torch.Size([1, 144, 144])']
[image_multi_fuser.0.dropout_modules.0] Output shape: torch.Size([144, 1, 288])
[image_multi_fuser.0.norm_modules.0] Output shape: torch.Size([144, 1, 288])
[image_multi_fuser.0.attn_modules.1] Output shapes (tuple): ['torch.Size([144, 1, 288])', 'torch.Size([1, 144, 144])']
[image_multi_fuser.0.dropout_modules.1] Output shape: torch.Size([144, 1, 288])
[image_multi_fuser.0.norm_modules.1] Output shape: torch.Size([144, 1, 288])
[image_multi_fuser.0.ffn.0] Output shape: torch.Size([144, 1, 256])
[image_multi_fuser.0.ffn.1] Output shape: torch.Size([144, 1, 256])
[image_multi_fuser.0.ffn.2] Output shape: torch.Size([144, 1, 256])
[image_multi_fuser.0.ffn.3] Output shape: torch.Size([144, 1, 288])
[image_multi_fuser.0.ffn.4] Output shape: torch.Size([144, 1, 288])
[image_multi_fuser.0.ffn] Output shape: torch.Size([144, 1, 288])
[image_multi_fuser.0.norm2] Output shape: torch.Size([144, 1, 288])
[image_multi_fuser.0] Output shape: torch.Size([1, 288, 144])
[pos_embed.position_embedding_head.0] Output shape: torch.Size([1, 288, 1024])
[pos_embed.position_embedding_head.1] Output shape: torch.Size([1, 288, 1024])
[pos_embed.position_embedding_head.2] Output shape: torch.Size([1, 288, 1024])
[pos_embed.position_embedding_head.3] Output shape: torch.Size([1, 288, 1024])
[pos_embed.position_embedding_head] Output shape: torch.Size([1, 288, 1024])
[pos_embed] Output shape: torch.Size([1, 288, 1024])
[cross_encoder_text_points.layers.0.self_attention_visual.self_attn] Output shapes (tuple): ['torch.Size([1024, 1, 288])', 'torch.Size([1, 1024, 1024])']
[cross_encoder_text_points.layers.0.self_attention_visual.dropout1] Output shape: torch.Size([1024, 1, 288])
[cross_encoder_text_points.layers.0.self_attention_visual.norm1] Output shape: torch.Size([1024, 1, 288])
[cross_encoder_text_points.layers.0.self_attention_visual] Output shape: torch.Size([1024, 1, 288])
[cross_encoder_text_points.layers.0.self_attention_lang.self_attn] Output shapes (tuple): ['torch.Size([33, 1, 288])', 'torch.Size([1, 33, 33])']
[cross_encoder_text_points.layers.0.self_attention_lang.dropout1] Output shape: torch.Size([33, 1, 288])
[cross_encoder_text_points.layers.0.self_attention_lang.norm1] Output shape: torch.Size([33, 1, 288])
[cross_encoder_text_points.layers.0.self_attention_lang] Output shape: torch.Size([33, 1, 288])
[cross_encoder_text_points.layers.0.cross_layer.cross_lv] Output shapes (tuple): ['torch.Size([33, 1, 288])', 'torch.Size([1, 33, 1024])']
[cross_encoder_text_points.layers.0.cross_layer.dropout_lv] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.0.cross_layer.norm_lv] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.0.cross_layer.ffn_lv.0] Output shape: torch.Size([1, 33, 256])
[cross_encoder_text_points.layers.0.cross_layer.ffn_lv.1] Output shape: torch.Size([1, 33, 256])
[cross_encoder_text_points.layers.0.cross_layer.ffn_lv.2] Output shape: torch.Size([1, 33, 256])
[cross_encoder_text_points.layers.0.cross_layer.ffn_lv.3] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.0.cross_layer.ffn_lv.4] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.0.cross_layer.ffn_lv] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.0.cross_layer.norm_lv2] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.0.cross_layer.cross_vl] Output shapes (tuple): ['torch.Size([1024, 1, 288])', 'torch.Size([1, 1024, 33])']
[cross_encoder_text_points.layers.0.cross_layer.dropout_vl] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.0.cross_layer.norm_vl] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.0.cross_layer.cross_d] Output shapes (tuple): ['torch.Size([1024, 1, 288])', 'torch.Size([1, 1024, 144])']
[cross_encoder_text_points.layers.0.cross_layer.dropout_d] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.0.cross_layer.norm_d] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.0.cross_layer.ffn_vl.0] Output shape: torch.Size([1, 1024, 256])
[cross_encoder_text_points.layers.0.cross_layer.ffn_vl.1] Output shape: torch.Size([1, 1024, 256])
[cross_encoder_text_points.layers.0.cross_layer.ffn_vl.2] Output shape: torch.Size([1, 1024, 256])
[cross_encoder_text_points.layers.0.cross_layer.ffn_vl.3] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.0.cross_layer.ffn_vl.4] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.0.cross_layer.ffn_vl] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.0.cross_layer.norm_vl2] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.0.cross_layer] Output shapes (tuple): ['torch.Size([1, 1024, 288])', 'torch.Size([1, 33, 288])']
[cross_encoder_text_points.layers.0] Output shapes (tuple): ['torch.Size([1, 1024, 288])', 'torch.Size([1, 33, 288])']
[cross_encoder_text_points.layers.1.self_attention_visual.self_attn] Output shapes (tuple): ['torch.Size([1024, 1, 288])', 'torch.Size([1, 1024, 1024])']
[cross_encoder_text_points.layers.1.self_attention_visual.dropout1] Output shape: torch.Size([1024, 1, 288])
[cross_encoder_text_points.layers.1.self_attention_visual.norm1] Output shape: torch.Size([1024, 1, 288])
[cross_encoder_text_points.layers.1.self_attention_visual] Output shape: torch.Size([1024, 1, 288])
[cross_encoder_text_points.layers.1.self_attention_lang.self_attn] Output shapes (tuple): ['torch.Size([33, 1, 288])', 'torch.Size([1, 33, 33])']
[cross_encoder_text_points.layers.1.self_attention_lang.dropout1] Output shape: torch.Size([33, 1, 288])
[cross_encoder_text_points.layers.1.self_attention_lang.norm1] Output shape: torch.Size([33, 1, 288])
[cross_encoder_text_points.layers.1.self_attention_lang] Output shape: torch.Size([33, 1, 288])
[cross_encoder_text_points.layers.1.cross_layer.cross_lv] Output shapes (tuple): ['torch.Size([33, 1, 288])', 'torch.Size([1, 33, 1024])']
[cross_encoder_text_points.layers.1.cross_layer.dropout_lv] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.1.cross_layer.norm_lv] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.1.cross_layer.ffn_lv.0] Output shape: torch.Size([1, 33, 256])
[cross_encoder_text_points.layers.1.cross_layer.ffn_lv.1] Output shape: torch.Size([1, 33, 256])
[cross_encoder_text_points.layers.1.cross_layer.ffn_lv.2] Output shape: torch.Size([1, 33, 256])
[cross_encoder_text_points.layers.1.cross_layer.ffn_lv.3] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.1.cross_layer.ffn_lv.4] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.1.cross_layer.ffn_lv] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.1.cross_layer.norm_lv2] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.1.cross_layer.cross_vl] Output shapes (tuple): ['torch.Size([1024, 1, 288])', 'torch.Size([1, 1024, 33])']
[cross_encoder_text_points.layers.1.cross_layer.dropout_vl] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.1.cross_layer.norm_vl] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.1.cross_layer.cross_d] Output shapes (tuple): ['torch.Size([1024, 1, 288])', 'torch.Size([1, 1024, 144])']
[cross_encoder_text_points.layers.1.cross_layer.dropout_d] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.1.cross_layer.norm_d] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.1.cross_layer.ffn_vl.0] Output shape: torch.Size([1, 1024, 256])
[cross_encoder_text_points.layers.1.cross_layer.ffn_vl.1] Output shape: torch.Size([1, 1024, 256])
[cross_encoder_text_points.layers.1.cross_layer.ffn_vl.2] Output shape: torch.Size([1, 1024, 256])
[cross_encoder_text_points.layers.1.cross_layer.ffn_vl.3] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.1.cross_layer.ffn_vl.4] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.1.cross_layer.ffn_vl] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.1.cross_layer.norm_vl2] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.1.cross_layer] Output shapes (tuple): ['torch.Size([1, 1024, 288])', 'torch.Size([1, 33, 288])']
[cross_encoder_text_points.layers.1] Output shapes (tuple): ['torch.Size([1, 1024, 288])', 'torch.Size([1, 33, 288])']
[cross_encoder_text_points.layers.2.self_attention_visual.self_attn] Output shapes (tuple): ['torch.Size([1024, 1, 288])', 'torch.Size([1, 1024, 1024])']
[cross_encoder_text_points.layers.2.self_attention_visual.dropout1] Output shape: torch.Size([1024, 1, 288])
[cross_encoder_text_points.layers.2.self_attention_visual.norm1] Output shape: torch.Size([1024, 1, 288])
[cross_encoder_text_points.layers.2.self_attention_visual] Output shape: torch.Size([1024, 1, 288])
[cross_encoder_text_points.layers.2.self_attention_lang.self_attn] Output shapes (tuple): ['torch.Size([33, 1, 288])', 'torch.Size([1, 33, 33])']
[cross_encoder_text_points.layers.2.self_attention_lang.dropout1] Output shape: torch.Size([33, 1, 288])
[cross_encoder_text_points.layers.2.self_attention_lang.norm1] Output shape: torch.Size([33, 1, 288])
[cross_encoder_text_points.layers.2.self_attention_lang] Output shape: torch.Size([33, 1, 288])
[cross_encoder_text_points.layers.2.cross_layer.cross_lv] Output shapes (tuple): ['torch.Size([33, 1, 288])', 'torch.Size([1, 33, 1024])']
[cross_encoder_text_points.layers.2.cross_layer.dropout_lv] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.2.cross_layer.norm_lv] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.2.cross_layer.ffn_lv.0] Output shape: torch.Size([1, 33, 256])
[cross_encoder_text_points.layers.2.cross_layer.ffn_lv.1] Output shape: torch.Size([1, 33, 256])
[cross_encoder_text_points.layers.2.cross_layer.ffn_lv.2] Output shape: torch.Size([1, 33, 256])
[cross_encoder_text_points.layers.2.cross_layer.ffn_lv.3] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.2.cross_layer.ffn_lv.4] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.2.cross_layer.ffn_lv] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.2.cross_layer.norm_lv2] Output shape: torch.Size([1, 33, 288])
[cross_encoder_text_points.layers.2.cross_layer.cross_vl] Output shapes (tuple): ['torch.Size([1024, 1, 288])', 'torch.Size([1, 1024, 33])']
[cross_encoder_text_points.layers.2.cross_layer.dropout_vl] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.2.cross_layer.norm_vl] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.2.cross_layer.cross_d] Output shapes (tuple): ['torch.Size([1024, 1, 288])', 'torch.Size([1, 1024, 144])']
[cross_encoder_text_points.layers.2.cross_layer.dropout_d] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.2.cross_layer.norm_d] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.2.cross_layer.ffn_vl.0] Output shape: torch.Size([1, 1024, 256])
[cross_encoder_text_points.layers.2.cross_layer.ffn_vl.1] Output shape: torch.Size([1, 1024, 256])
[cross_encoder_text_points.layers.2.cross_layer.ffn_vl.2] Output shape: torch.Size([1, 1024, 256])
[cross_encoder_text_points.layers.2.cross_layer.ffn_vl.3] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.2.cross_layer.ffn_vl.4] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.2.cross_layer.ffn_vl] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.2.cross_layer.norm_vl2] Output shape: torch.Size([1, 1024, 288])
[cross_encoder_text_points.layers.2.cross_layer] Output shapes (tuple): ['torch.Size([1, 1024, 288])', 'torch.Size([1, 33, 288])']
[cross_encoder_text_points.layers.2] Output shapes (tuple): ['torch.Size([1, 1024, 288])', 'torch.Size([1, 33, 288])']
[cross_encoder_text_points] Output shapes (tuple): ['torch.Size([1, 1024, 288])', 'torch.Size([1, 33, 288])']
[contrastive_align_projection_text.0] Output shape: torch.Size([1, 33, 288])
[contrastive_align_projection_text.1] Output shape: torch.Size([1, 33, 288])
[contrastive_align_projection_text.2] Output shape: torch.Size([1, 33, 288])
[contrastive_align_projection_text.3] Output shape: torch.Size([1, 33, 288])
[contrastive_align_projection_text.4] Output shape: torch.Size([1, 33, 64])
[contrastive_align_projection_text] Output shape: torch.Size([1, 33, 64])
[points_obj_cls.conv1] Output shape: torch.Size([1, 288, 1024])
[points_obj_cls.bn1] Output shape: torch.Size([1, 288, 1024])
[points_obj_cls.conv2] Output shape: torch.Size([1, 288, 1024])
[points_obj_cls.bn2] Output shape: torch.Size([1, 288, 1024])
[points_obj_cls.conv3] Output shape: torch.Size([1, 1, 1024])
[points_obj_cls] Output shape: torch.Size([1, 1, 1024])
[gsample_module] Output shapes (tuple): ['torch.Size([1, 256, 3])', 'torch.Size([1, 288, 256])', 'torch.Size([1, 256])']
[decoder_query_proj] Output shape: torch.Size([1, 288, 256])
[contrastive_align_projection_image.0] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.1] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.2] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.3] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.4] Output shape: torch.Size([1, 256, 64])
[contrastive_align_projection_image] Output shape: torch.Size([1, 256, 64])
[proposal_head.center_residual_head.net.0] Output shape: torch.Size([1, 288, 256])
[proposal_head.center_residual_head.net.1] Output shape: torch.Size([1, 288, 256])
[proposal_head.center_residual_head.net.2] Output shape: torch.Size([1, 288, 256])
[proposal_head.center_residual_head.net.3] Output shape: torch.Size([1, 288, 256])
[proposal_head.center_residual_head.net.4] Output shape: torch.Size([1, 288, 256])
[proposal_head.center_residual_head.net.5] Output shape: torch.Size([1, 288, 256])
[proposal_head.center_residual_head.net.6] Output shape: torch.Size([1, 288, 256])
[proposal_head.center_residual_head.net.7] Output shape: torch.Size([1, 288, 256])
[proposal_head.center_residual_head.net.8] Output shape: torch.Size([1, 3, 256])
[proposal_head.center_residual_head.net] Output shape: torch.Size([1, 3, 256])
[proposal_head.center_residual_head] Output shape: torch.Size([1, 3, 256])
[proposal_head.size_pred_head.net.0] Output shape: torch.Size([1, 288, 256])
[proposal_head.size_pred_head.net.1] Output shape: torch.Size([1, 288, 256])
[proposal_head.size_pred_head.net.2] Output shape: torch.Size([1, 288, 256])
[proposal_head.size_pred_head.net.3] Output shape: torch.Size([1, 288, 256])
[proposal_head.size_pred_head.net.4] Output shape: torch.Size([1, 288, 256])
[proposal_head.size_pred_head.net.5] Output shape: torch.Size([1, 288, 256])
[proposal_head.size_pred_head.net.6] Output shape: torch.Size([1, 288, 256])
[proposal_head.size_pred_head.net.7] Output shape: torch.Size([1, 288, 256])
[proposal_head.size_pred_head.net.8] Output shape: torch.Size([1, 3, 256])
[proposal_head.size_pred_head.net] Output shape: torch.Size([1, 3, 256])
[proposal_head.size_pred_head] Output shape: torch.Size([1, 3, 256])
[proposal_head.sem_cls_scores_head.net.0] Output shape: torch.Size([1, 288, 256])
[proposal_head.sem_cls_scores_head.net.1] Output shape: torch.Size([1, 288, 256])
[proposal_head.sem_cls_scores_head.net.2] Output shape: torch.Size([1, 288, 256])
[proposal_head.sem_cls_scores_head.net.3] Output shape: torch.Size([1, 288, 256])
[proposal_head.sem_cls_scores_head.net.4] Output shape: torch.Size([1, 288, 256])
[proposal_head.sem_cls_scores_head.net.5] Output shape: torch.Size([1, 288, 256])
[proposal_head.sem_cls_scores_head.net.6] Output shape: torch.Size([1, 288, 256])
[proposal_head.sem_cls_scores_head.net.7] Output shape: torch.Size([1, 288, 256])
[proposal_head.sem_cls_scores_head.net.8] Output shape: torch.Size([1, 100, 256])
[proposal_head.sem_cls_scores_head.net] Output shape: torch.Size([1, 100, 256])
[proposal_head.sem_cls_scores_head] Output shape: torch.Size([1, 100, 256])
[proposal_head] Output shapes (tuple): ['torch.Size([1, 256, 3])', 'torch.Size([1, 256, 3])']
[decoder.0.self_posembed.position_embedding_head.0] Output shape: torch.Size([1, 288, 256])
[decoder.0.self_posembed.position_embedding_head.1] Output shape: torch.Size([1, 288, 256])
[decoder.0.self_posembed.position_embedding_head.2] Output shape: torch.Size([1, 288, 256])
[decoder.0.self_posembed.position_embedding_head.3] Output shape: torch.Size([1, 288, 256])
[decoder.0.self_posembed.position_embedding_head] Output shape: torch.Size([1, 288, 256])
[decoder.0.self_posembed] Output shape: torch.Size([1, 288, 256])
[decoder.0.self_attn] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 256])']
[decoder.0.dropout1] Output shape: torch.Size([256, 1, 288])
[decoder.0.norm1] Output shape: torch.Size([256, 1, 288])
[decoder.0.cross_l] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 33])']
[decoder.0.dropout_l] Output shape: torch.Size([256, 1, 288])
[decoder.0.norm_l] Output shape: torch.Size([256, 1, 288])
[decoder.0.cross_v] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 1024])']
[decoder.0.dropout_v] Output shape: torch.Size([256, 1, 288])
[decoder.0.norm_v] Output shape: torch.Size([256, 1, 288])
[decoder.0.ffn.0] Output shape: torch.Size([256, 1, 256])
[decoder.0.ffn.1] Output shape: torch.Size([256, 1, 256])
[decoder.0.ffn.2] Output shape: torch.Size([256, 1, 256])
[decoder.0.ffn.3] Output shape: torch.Size([256, 1, 288])
[decoder.0.ffn.4] Output shape: torch.Size([256, 1, 288])
[decoder.0.ffn] Output shape: torch.Size([256, 1, 288])
[decoder.0.norm2] Output shape: torch.Size([256, 1, 288])
[decoder.0] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.0] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.1] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.2] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.3] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.4] Output shape: torch.Size([1, 256, 64])
[contrastive_align_projection_image] Output shape: torch.Size([1, 256, 64])
[prediction_heads.0.center_residual_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.center_residual_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.center_residual_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.center_residual_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.center_residual_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.center_residual_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.center_residual_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.center_residual_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.center_residual_head.net.8] Output shape: torch.Size([1, 3, 256])
[prediction_heads.0.center_residual_head.net] Output shape: torch.Size([1, 3, 256])
[prediction_heads.0.center_residual_head] Output shape: torch.Size([1, 3, 256])
[prediction_heads.0.size_pred_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.size_pred_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.size_pred_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.size_pred_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.size_pred_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.size_pred_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.size_pred_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.size_pred_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.size_pred_head.net.8] Output shape: torch.Size([1, 3, 256])
[prediction_heads.0.size_pred_head.net] Output shape: torch.Size([1, 3, 256])
[prediction_heads.0.size_pred_head] Output shape: torch.Size([1, 3, 256])
[prediction_heads.0.sem_cls_scores_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.sem_cls_scores_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.sem_cls_scores_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.sem_cls_scores_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.sem_cls_scores_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.sem_cls_scores_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.sem_cls_scores_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.sem_cls_scores_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.0.sem_cls_scores_head.net.8] Output shape: torch.Size([1, 100, 256])
[prediction_heads.0.sem_cls_scores_head.net] Output shape: torch.Size([1, 100, 256])
[prediction_heads.0.sem_cls_scores_head] Output shape: torch.Size([1, 100, 256])
[prediction_heads.0] Output shapes (tuple): ['torch.Size([1, 256, 3])', 'torch.Size([1, 256, 3])']
[decoder.1.self_posembed.position_embedding_head.0] Output shape: torch.Size([1, 288, 256])
[decoder.1.self_posembed.position_embedding_head.1] Output shape: torch.Size([1, 288, 256])
[decoder.1.self_posembed.position_embedding_head.2] Output shape: torch.Size([1, 288, 256])
[decoder.1.self_posembed.position_embedding_head.3] Output shape: torch.Size([1, 288, 256])
[decoder.1.self_posembed.position_embedding_head] Output shape: torch.Size([1, 288, 256])
[decoder.1.self_posembed] Output shape: torch.Size([1, 288, 256])
[decoder.1.self_attn] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 256])']
[decoder.1.dropout1] Output shape: torch.Size([256, 1, 288])
[decoder.1.norm1] Output shape: torch.Size([256, 1, 288])
[decoder.1.cross_l] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 33])']
[decoder.1.dropout_l] Output shape: torch.Size([256, 1, 288])
[decoder.1.norm_l] Output shape: torch.Size([256, 1, 288])
[decoder.1.cross_v] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 1024])']
[decoder.1.dropout_v] Output shape: torch.Size([256, 1, 288])
[decoder.1.norm_v] Output shape: torch.Size([256, 1, 288])
[decoder.1.ffn.0] Output shape: torch.Size([256, 1, 256])
[decoder.1.ffn.1] Output shape: torch.Size([256, 1, 256])
[decoder.1.ffn.2] Output shape: torch.Size([256, 1, 256])
[decoder.1.ffn.3] Output shape: torch.Size([256, 1, 288])
[decoder.1.ffn.4] Output shape: torch.Size([256, 1, 288])
[decoder.1.ffn] Output shape: torch.Size([256, 1, 288])
[decoder.1.norm2] Output shape: torch.Size([256, 1, 288])
[decoder.1] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.0] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.1] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.2] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.3] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.4] Output shape: torch.Size([1, 256, 64])
[contrastive_align_projection_image] Output shape: torch.Size([1, 256, 64])
[prediction_heads.1.center_residual_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.center_residual_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.center_residual_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.center_residual_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.center_residual_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.center_residual_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.center_residual_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.center_residual_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.center_residual_head.net.8] Output shape: torch.Size([1, 3, 256])
[prediction_heads.1.center_residual_head.net] Output shape: torch.Size([1, 3, 256])
[prediction_heads.1.center_residual_head] Output shape: torch.Size([1, 3, 256])
[prediction_heads.1.size_pred_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.size_pred_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.size_pred_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.size_pred_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.size_pred_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.size_pred_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.size_pred_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.size_pred_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.size_pred_head.net.8] Output shape: torch.Size([1, 3, 256])
[prediction_heads.1.size_pred_head.net] Output shape: torch.Size([1, 3, 256])
[prediction_heads.1.size_pred_head] Output shape: torch.Size([1, 3, 256])
[prediction_heads.1.sem_cls_scores_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.sem_cls_scores_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.sem_cls_scores_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.sem_cls_scores_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.sem_cls_scores_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.sem_cls_scores_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.sem_cls_scores_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.sem_cls_scores_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.1.sem_cls_scores_head.net.8] Output shape: torch.Size([1, 100, 256])
[prediction_heads.1.sem_cls_scores_head.net] Output shape: torch.Size([1, 100, 256])
[prediction_heads.1.sem_cls_scores_head] Output shape: torch.Size([1, 100, 256])
[prediction_heads.1] Output shapes (tuple): ['torch.Size([1, 256, 3])', 'torch.Size([1, 256, 3])']
[decoder.2.self_posembed.position_embedding_head.0] Output shape: torch.Size([1, 288, 256])
[decoder.2.self_posembed.position_embedding_head.1] Output shape: torch.Size([1, 288, 256])
[decoder.2.self_posembed.position_embedding_head.2] Output shape: torch.Size([1, 288, 256])
[decoder.2.self_posembed.position_embedding_head.3] Output shape: torch.Size([1, 288, 256])
[decoder.2.self_posembed.position_embedding_head] Output shape: torch.Size([1, 288, 256])
[decoder.2.self_posembed] Output shape: torch.Size([1, 288, 256])
[decoder.2.self_attn] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 256])']
[decoder.2.dropout1] Output shape: torch.Size([256, 1, 288])
[decoder.2.norm1] Output shape: torch.Size([256, 1, 288])
[decoder.2.cross_l] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 33])']
[decoder.2.dropout_l] Output shape: torch.Size([256, 1, 288])
[decoder.2.norm_l] Output shape: torch.Size([256, 1, 288])
[decoder.2.cross_v] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 1024])']
[decoder.2.dropout_v] Output shape: torch.Size([256, 1, 288])
[decoder.2.norm_v] Output shape: torch.Size([256, 1, 288])
[decoder.2.ffn.0] Output shape: torch.Size([256, 1, 256])
[decoder.2.ffn.1] Output shape: torch.Size([256, 1, 256])
[decoder.2.ffn.2] Output shape: torch.Size([256, 1, 256])
[decoder.2.ffn.3] Output shape: torch.Size([256, 1, 288])
[decoder.2.ffn.4] Output shape: torch.Size([256, 1, 288])
[decoder.2.ffn] Output shape: torch.Size([256, 1, 288])
[decoder.2.norm2] Output shape: torch.Size([256, 1, 288])
[decoder.2] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.0] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.1] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.2] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.3] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.4] Output shape: torch.Size([1, 256, 64])
[contrastive_align_projection_image] Output shape: torch.Size([1, 256, 64])
[prediction_heads.2.center_residual_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.center_residual_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.center_residual_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.center_residual_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.center_residual_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.center_residual_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.center_residual_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.center_residual_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.center_residual_head.net.8] Output shape: torch.Size([1, 3, 256])
[prediction_heads.2.center_residual_head.net] Output shape: torch.Size([1, 3, 256])
[prediction_heads.2.center_residual_head] Output shape: torch.Size([1, 3, 256])
[prediction_heads.2.size_pred_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.size_pred_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.size_pred_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.size_pred_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.size_pred_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.size_pred_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.size_pred_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.size_pred_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.size_pred_head.net.8] Output shape: torch.Size([1, 3, 256])
[prediction_heads.2.size_pred_head.net] Output shape: torch.Size([1, 3, 256])
[prediction_heads.2.size_pred_head] Output shape: torch.Size([1, 3, 256])
[prediction_heads.2.sem_cls_scores_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.sem_cls_scores_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.sem_cls_scores_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.sem_cls_scores_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.sem_cls_scores_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.sem_cls_scores_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.sem_cls_scores_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.sem_cls_scores_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.2.sem_cls_scores_head.net.8] Output shape: torch.Size([1, 100, 256])
[prediction_heads.2.sem_cls_scores_head.net] Output shape: torch.Size([1, 100, 256])
[prediction_heads.2.sem_cls_scores_head] Output shape: torch.Size([1, 100, 256])
[prediction_heads.2] Output shapes (tuple): ['torch.Size([1, 256, 3])', 'torch.Size([1, 256, 3])']
[decoder.3.self_posembed.position_embedding_head.0] Output shape: torch.Size([1, 288, 256])
[decoder.3.self_posembed.position_embedding_head.1] Output shape: torch.Size([1, 288, 256])
[decoder.3.self_posembed.position_embedding_head.2] Output shape: torch.Size([1, 288, 256])
[decoder.3.self_posembed.position_embedding_head.3] Output shape: torch.Size([1, 288, 256])
[decoder.3.self_posembed.position_embedding_head] Output shape: torch.Size([1, 288, 256])
[decoder.3.self_posembed] Output shape: torch.Size([1, 288, 256])
[decoder.3.self_attn] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 256])']
[decoder.3.dropout1] Output shape: torch.Size([256, 1, 288])
[decoder.3.norm1] Output shape: torch.Size([256, 1, 288])
[decoder.3.cross_l] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 33])']
[decoder.3.dropout_l] Output shape: torch.Size([256, 1, 288])
[decoder.3.norm_l] Output shape: torch.Size([256, 1, 288])
[decoder.3.cross_v] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 1024])']
[decoder.3.dropout_v] Output shape: torch.Size([256, 1, 288])
[decoder.3.norm_v] Output shape: torch.Size([256, 1, 288])
[decoder.3.ffn.0] Output shape: torch.Size([256, 1, 256])
[decoder.3.ffn.1] Output shape: torch.Size([256, 1, 256])
[decoder.3.ffn.2] Output shape: torch.Size([256, 1, 256])
[decoder.3.ffn.3] Output shape: torch.Size([256, 1, 288])
[decoder.3.ffn.4] Output shape: torch.Size([256, 1, 288])
[decoder.3.ffn] Output shape: torch.Size([256, 1, 288])
[decoder.3.norm2] Output shape: torch.Size([256, 1, 288])
[decoder.3] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.0] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.1] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.2] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.3] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.4] Output shape: torch.Size([1, 256, 64])
[contrastive_align_projection_image] Output shape: torch.Size([1, 256, 64])
[prediction_heads.3.center_residual_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.center_residual_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.center_residual_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.center_residual_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.center_residual_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.center_residual_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.center_residual_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.center_residual_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.center_residual_head.net.8] Output shape: torch.Size([1, 3, 256])
[prediction_heads.3.center_residual_head.net] Output shape: torch.Size([1, 3, 256])
[prediction_heads.3.center_residual_head] Output shape: torch.Size([1, 3, 256])
[prediction_heads.3.size_pred_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.size_pred_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.size_pred_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.size_pred_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.size_pred_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.size_pred_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.size_pred_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.size_pred_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.size_pred_head.net.8] Output shape: torch.Size([1, 3, 256])
[prediction_heads.3.size_pred_head.net] Output shape: torch.Size([1, 3, 256])
[prediction_heads.3.size_pred_head] Output shape: torch.Size([1, 3, 256])
[prediction_heads.3.sem_cls_scores_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.sem_cls_scores_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.sem_cls_scores_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.sem_cls_scores_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.sem_cls_scores_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.sem_cls_scores_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.sem_cls_scores_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.sem_cls_scores_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.3.sem_cls_scores_head.net.8] Output shape: torch.Size([1, 100, 256])
[prediction_heads.3.sem_cls_scores_head.net] Output shape: torch.Size([1, 100, 256])
[prediction_heads.3.sem_cls_scores_head] Output shape: torch.Size([1, 100, 256])
[prediction_heads.3] Output shapes (tuple): ['torch.Size([1, 256, 3])', 'torch.Size([1, 256, 3])']
[decoder.4.self_posembed.position_embedding_head.0] Output shape: torch.Size([1, 288, 256])
[decoder.4.self_posembed.position_embedding_head.1] Output shape: torch.Size([1, 288, 256])
[decoder.4.self_posembed.position_embedding_head.2] Output shape: torch.Size([1, 288, 256])
[decoder.4.self_posembed.position_embedding_head.3] Output shape: torch.Size([1, 288, 256])
[decoder.4.self_posembed.position_embedding_head] Output shape: torch.Size([1, 288, 256])
[decoder.4.self_posembed] Output shape: torch.Size([1, 288, 256])
[decoder.4.self_attn] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 256])']
[decoder.4.dropout1] Output shape: torch.Size([256, 1, 288])
[decoder.4.norm1] Output shape: torch.Size([256, 1, 288])
[decoder.4.cross_l] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 33])']
[decoder.4.dropout_l] Output shape: torch.Size([256, 1, 288])
[decoder.4.norm_l] Output shape: torch.Size([256, 1, 288])
[decoder.4.cross_v] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 1024])']
[decoder.4.dropout_v] Output shape: torch.Size([256, 1, 288])
[decoder.4.norm_v] Output shape: torch.Size([256, 1, 288])
[decoder.4.ffn.0] Output shape: torch.Size([256, 1, 256])
[decoder.4.ffn.1] Output shape: torch.Size([256, 1, 256])
[decoder.4.ffn.2] Output shape: torch.Size([256, 1, 256])
[decoder.4.ffn.3] Output shape: torch.Size([256, 1, 288])
[decoder.4.ffn.4] Output shape: torch.Size([256, 1, 288])
[decoder.4.ffn] Output shape: torch.Size([256, 1, 288])
[decoder.4.norm2] Output shape: torch.Size([256, 1, 288])
[decoder.4] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.0] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.1] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.2] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.3] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.4] Output shape: torch.Size([1, 256, 64])
[contrastive_align_projection_image] Output shape: torch.Size([1, 256, 64])
[prediction_heads.4.center_residual_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.center_residual_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.center_residual_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.center_residual_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.center_residual_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.center_residual_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.center_residual_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.center_residual_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.center_residual_head.net.8] Output shape: torch.Size([1, 3, 256])
[prediction_heads.4.center_residual_head.net] Output shape: torch.Size([1, 3, 256])
[prediction_heads.4.center_residual_head] Output shape: torch.Size([1, 3, 256])
[prediction_heads.4.size_pred_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.size_pred_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.size_pred_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.size_pred_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.size_pred_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.size_pred_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.size_pred_head.net.6] Output shape: torch.Size([1, 288, 256])
/home/avishka/sasika/WildRefer/models/position_encoding.py:37: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)
[prediction_heads.4.size_pred_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.size_pred_head.net.8] Output shape: torch.Size([1, 3, 256])
[prediction_heads.4.size_pred_head.net] Output shape: torch.Size([1, 3, 256])
[prediction_heads.4.size_pred_head] Output shape: torch.Size([1, 3, 256])
[prediction_heads.4.sem_cls_scores_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.sem_cls_scores_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.sem_cls_scores_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.sem_cls_scores_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.sem_cls_scores_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.sem_cls_scores_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.sem_cls_scores_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.sem_cls_scores_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.4.sem_cls_scores_head.net.8] Output shape: torch.Size([1, 100, 256])
[prediction_heads.4.sem_cls_scores_head.net] Output shape: torch.Size([1, 100, 256])
[prediction_heads.4.sem_cls_scores_head] Output shape: torch.Size([1, 100, 256])
[prediction_heads.4] Output shapes (tuple): ['torch.Size([1, 256, 3])', 'torch.Size([1, 256, 3])']
[decoder.5.self_posembed.position_embedding_head.0] Output shape: torch.Size([1, 288, 256])
[decoder.5.self_posembed.position_embedding_head.1] Output shape: torch.Size([1, 288, 256])
[decoder.5.self_posembed.position_embedding_head.2] Output shape: torch.Size([1, 288, 256])
[decoder.5.self_posembed.position_embedding_head.3] Output shape: torch.Size([1, 288, 256])
[decoder.5.self_posembed.position_embedding_head] Output shape: torch.Size([1, 288, 256])
[decoder.5.self_posembed] Output shape: torch.Size([1, 288, 256])
[decoder.5.self_attn] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 256])']
[decoder.5.dropout1] Output shape: torch.Size([256, 1, 288])
[decoder.5.norm1] Output shape: torch.Size([256, 1, 288])
[decoder.5.cross_l] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 33])']
[decoder.5.dropout_l] Output shape: torch.Size([256, 1, 288])
[decoder.5.norm_l] Output shape: torch.Size([256, 1, 288])
[decoder.5.cross_v] Output shapes (tuple): ['torch.Size([256, 1, 288])', 'torch.Size([1, 256, 1024])']
[decoder.5.dropout_v] Output shape: torch.Size([256, 1, 288])
[decoder.5.norm_v] Output shape: torch.Size([256, 1, 288])
[decoder.5.ffn.0] Output shape: torch.Size([256, 1, 256])
[decoder.5.ffn.1] Output shape: torch.Size([256, 1, 256])
[decoder.5.ffn.2] Output shape: torch.Size([256, 1, 256])
[decoder.5.ffn.3] Output shape: torch.Size([256, 1, 288])
[decoder.5.ffn.4] Output shape: torch.Size([256, 1, 288])
[decoder.5.ffn] Output shape: torch.Size([256, 1, 288])
[decoder.5.norm2] Output shape: torch.Size([256, 1, 288])
[decoder.5] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.0] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.1] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.2] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.3] Output shape: torch.Size([1, 256, 288])
[contrastive_align_projection_image.4] Output shape: torch.Size([1, 256, 64])
[contrastive_align_projection_image] Output shape: torch.Size([1, 256, 64])
[prediction_heads.5.center_residual_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.center_residual_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.center_residual_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.center_residual_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.center_residual_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.center_residual_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.center_residual_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.center_residual_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.center_residual_head.net.8] Output shape: torch.Size([1, 3, 256])
[prediction_heads.5.center_residual_head.net] Output shape: torch.Size([1, 3, 256])
[prediction_heads.5.center_residual_head] Output shape: torch.Size([1, 3, 256])
[prediction_heads.5.size_pred_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.size_pred_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.size_pred_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.size_pred_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.size_pred_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.size_pred_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.size_pred_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.size_pred_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.size_pred_head.net.8] Output shape: torch.Size([1, 3, 256])
[prediction_heads.5.size_pred_head.net] Output shape: torch.Size([1, 3, 256])
[prediction_heads.5.size_pred_head] Output shape: torch.Size([1, 3, 256])
[prediction_heads.5.sem_cls_scores_head.net.0] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.sem_cls_scores_head.net.1] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.sem_cls_scores_head.net.2] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.sem_cls_scores_head.net.3] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.sem_cls_scores_head.net.4] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.sem_cls_scores_head.net.5] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.sem_cls_scores_head.net.6] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.sem_cls_scores_head.net.7] Output shape: torch.Size([1, 288, 256])
[prediction_heads.5.sem_cls_scores_head.net.8] Output shape: torch.Size([1, 100, 256])
[prediction_heads.5.sem_cls_scores_head.net] Output shape: torch.Size([1, 100, 256])
[prediction_heads.5.sem_cls_scores_head] Output shape: torch.Size([1, 100, 256])
[prediction_heads.5] Output shapes (tuple): ['torch.Size([1, 256, 3])', 'torch.Size([1, 256, 3])']

================================================================================
FORWARD PASS COMPLETED
================================================================================

[Difference of keys from inputs to end_points]
{'0head_base_xyz',
 '0head_center',
 '0head_pred_size',
 '0head_proj_queries',
 '0head_sem_cls_scores',
 '1head_base_xyz',
 '1head_center',
 '1head_pred_size',
 '1head_proj_queries',
 '1head_sem_cls_scores',
 '2head_base_xyz',
 '2head_center',
 '2head_pred_size',
 '2head_proj_queries',
 '2head_sem_cls_scores',
 '3head_base_xyz',
 '3head_center',
 '3head_pred_size',
 '3head_proj_queries',
 '3head_sem_cls_scores',
 '4head_base_xyz',
 '4head_center',
 '4head_pred_size',
 '4head_proj_queries',
 '4head_sem_cls_scores',
 'additional_image_feature',
 'additional_img_mask',
 'additional_img_pos',
 'additional_seed_features',
 'additional_seed_inds',
 'additional_seed_xyz',
 'fp2_features',
 'fp2_inds',
 'fp2_xyz',
 'image_feature',
 'img_pos',
 'last_base_xyz',
 'last_center',
 'last_pred_size',
 'last_proj_queries',
 'last_sem_cls_scores',
 'proj_tokens',
 'proposal_base_xyz',
 'proposal_center',
 'proposal_pred_size',
 'proposal_proj_queries',
 'proposal_sem_cls_scores',
 'query_points_feature',
 'query_points_sample_inds',
 'query_points_xyz',
 'sa1_features',
 'sa1_inds',
 'sa1_xyz',
 'sa2_features',
 'sa2_inds',
 'sa2_xyz',
 'sa3_features',
 'sa3_xyz',
 'sa4_features',
 'sa4_xyz',
 'seed_features',
 'seed_inds',
 'seed_xyz',
 'seeds_obj_cls_logits',
 'text_attention_mask',
 'text_feats',
 'text_memory',
 'tokenized'}
  0%|          | 0/1 [00:00<?, ?it/s]100%|| 1/1 [00:00<00:00, 45590.26it/s]
acc25=0.0
acc50=0.0
m_iou=0.0
